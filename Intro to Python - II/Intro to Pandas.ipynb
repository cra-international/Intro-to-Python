{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Part II: Intro to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python Part I, we flew through introductory Python concepts, everything from the `print()` statement to For Loops, and used that information to write cool little applications like simulating a coin flip. At the end, we covered packages, which are importable Python obejcts with defined methods and attributes. We looked at the `math` package, we includes relevant mathmatical functions like `log10()` and `gcd()`. We also looked at the `random` package, with allowed us to create random variables. \n",
    "\n",
    "We will now introduce the Pandas package, which is a Python package that allows users easily to manipulate datasets in Python.\n",
    "\n",
    "*Note: In the walkthrough below, the first instance of each new Pandas method or attribute will be hyperlinked directly to the Pandas documentation for that method/attribute. Feel free to reference that documentation for additional information.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to start using Pandas, we have to import it. Typically, Pandas is imported as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code imports Pandas and then assigns the package a nickname, in this case `pd`. Therefore, whenever we refer to Pandas we can just type `pd`. \n",
    "\n",
    "Below we use Pandas to read in our first dataset - the `cookie_sales.csv` file stored in the \"Intro to Python - II\" repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_url = r\"https://raw.githubusercontent.com/cra-international/Intro-to-Python/master/Intro%20to%20Python%20-%20II/cookie_sales.csv\"\n",
    "sales_df = pd.read_csv(sales_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we reference the Pandas package using the `pd` notation discussed above. We then use Pandas to call the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function, which has one *required* argument, in this case the path to the file. As the documentation notes, URLs are valid filepaths. So in this case, we pass a URL to the function which points to the CSV file stored on GitHub.\n",
    "\n",
    "Great! So, what was returned by this `read_csv()` function? We can use Python's built-in `type()` function to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sales_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, what was returned was a *DataFrame*, which is a data type we haven't seen before. That's because DataFrames are defined by (and specific to) the Pandas package. The majority of this demo will focus on learning more about Pandas DataFrames, as this is where all the magic happens. DataFrames not only store data, but they also have *methods* that allow us to manipulate the data easily. We'll cover the methods in more detail, and exactly how DataFrames store data, below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you try! \n",
    "\n",
    "Create a new dataframe, called `price_df`, that takes the value of the data stored in the `cookie_prices.csv` file. The URL to that file has already been stored in the `price_url` variable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_url = r\"https://raw.githubusercontent.com/cra-international/Intro-to-Python/master/Intro%20to%20Python%20-%20II/cookie_prices.csv\"\n",
    "price_df = pd.read_csv(price_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Exploring Data\n",
    "\n",
    "Now that we can read in the data, the next step is to explore the data and see what it looks like. So let's learn our first Pandas function. We can get a quick glimpse of the data by using the [`.head()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) method, which by default prints the first five observations of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that there are 4 columns - Troop Number, Cookie, state, Boxes Sold, and Notes. Based on the looks of it, this looks like Girl Scout Cookie Sales data by troop and cookie type.\n",
    "\n",
    "We can vary the number of observations `.head()` returns by passing the specific number of observations we want to see to the function. For example, we can print the first 8 observation by running the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Troop Number</th>\n",
       "      <th>Box</th>\n",
       "      <th>Sold</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179</td>\n",
       "      <td>Thin Mints</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179</td>\n",
       "      <td>Caramel deLites</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179</td>\n",
       "      <td>Peanut Butter Patties</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>179</td>\n",
       "      <td>Girl Scout S'mores</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>179</td>\n",
       "      <td>Lemonades</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>179</td>\n",
       "      <td>Peanut Butter Sandwich</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>179</td>\n",
       "      <td>Shortbread</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>179</td>\n",
       "      <td>Thanks-A-Lot</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Troop Number                     Box Sold Notes\n",
       "0           179              Thin Mints   43   NaN\n",
       "1           179         Caramel deLites   16   NaN\n",
       "2           179   Peanut Butter Patties   22   NaN\n",
       "3           179      Girl Scout S'mores   11   NaN\n",
       "4           179               Lemonades   34   NaN\n",
       "5           179  Peanut Butter Sandwich    8   NaN\n",
       "6           179              Shortbread   38   NaN\n",
       "7           179            Thanks-A-Lot   26   NaN"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the shape of the data using the [`.shape`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html) attribute. We have not covered attributes up to this point, but an *attribute* is a static value associated with a Python object. Like methods, attributes are accessed using the dot operator, however they are not followed by any parentheses. In this case, the `.shape` attribute of a Pandas DataFrame returns a *tuple* of the number of observations and number of columns in the DataFrame, in that order. We will talk more about tuples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first value of the tuple returned by `.shape` is `40` and the second value is `4`, indicating that there are 40 observations and 4 columns in `sales_df`. Tuples are very simlar to lists as they are ordered and therefore can be accessed using positional indexing. For example, we can get just the number of observations of `sales_df` by doing the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sales_df has\", sales_df.shape[0] , \"observations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between tuples and lists, however, is that tuples are *not* mutable, meaning you can not change the values of a tuple using positional indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now you try!\n",
    "\n",
    "In the first code cell below, use the `.head()` method to look at the first 6 observations of the `price_df` DataFrame you created above. What does the `price_df` dataset look like? What kind of data is it? Then, in the second code cell, use the `.shape` attribute of `price_df` print the number of columns of the data to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"price_df has\", price_df.shape[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Pandas Data Types</ins>\n",
    "\n",
    "DataFrames, like other 2-dimensional data storage types such as SQL tables or Stata datasets, defines data types as the *column* level. Therefore, like regular Python, Dataframes also have basic data types we need to learn about. These are \n",
    "* int64\n",
    "* float64\n",
    "* object\n",
    "* bool\n",
    "* datetime64\n",
    "* NaN/nan\n",
    "\n",
    "You should notice some similarities between these data types and the basic Python datatypes. `int64` is Pandas' integer data type, where the 64 denotes how many bits each value is comprised of. Since there are 8 bits in a byte, that means the largest value an int64 can be is $2^{64}$, which is typically plenty large enough. `float64` is simlar, but for floats and `bool` is a column of boolean values. \n",
    "\n",
    "How are strings stored? Strings are stored using the `object` data type. It's important to note that while all string columns are stored as `object`s, not all `object` columns are strings. For example, we may have a column of *mixed* values, say strings and numbers. That type of column will be cast as an `object` type, essentially converting the numbers in column to text.\n",
    "\n",
    "This is one additional type of datatype we haven't encountered before, and that's the `datetime64` type. `datetime` objects are built-in Python storage objects that allow for easy creation, manipulation, and storage of dates. Pandas' `datetime64` data type is a vectorized form of of the `datetime` object type which allocates 8 bytes to each `datetime` value in the column. Unfortunately we won't talk much about the `datetime` column type today.\n",
    "\n",
    "Finally, `NaN` is not a data type *per se*, however it stands for \"Not a Number\" and is how Pandas represents [missing data](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html). Note that while `NaN` stands for \"Not a Number\", it is used in string/object data types, for example the \"Notes\" column in `sales_df` seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we see what data types specific columns are in our data? We can do that by referencing the [`.dtypes`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) attribute of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Troop Number     int64\n",
       "Box             object\n",
       "Sold            object\n",
       "Notes           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame attribute simply returns as text the data type of each column in the associated DataFrame.\n",
    "\n",
    "Based on the first couple observations of the data we saw from calling the `.head()` method, it's a bit odd that \"Boxes Sold\", which looks numeric, is of the `object` type. What's going on? Let's print the whole dataset to see. To do that in a Jupyter Notebook, we simply run a block of code with just the variable name of the DataFrame we want to see, as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Troop Number</th>\n",
       "      <th>Box</th>\n",
       "      <th>Sold</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179</td>\n",
       "      <td>Thin Mints</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179</td>\n",
       "      <td>Caramel deLites</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>179</td>\n",
       "      <td>Peanut Butter Patties</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>179</td>\n",
       "      <td>Girl Scout S'mores</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>179</td>\n",
       "      <td>Lemonades</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>179</td>\n",
       "      <td>Peanut Butter Sandwich</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>179</td>\n",
       "      <td>Shortbread</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>179</td>\n",
       "      <td>Thanks-A-Lot</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>179</td>\n",
       "      <td>Caramel Chocolate Chip</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>179</td>\n",
       "      <td>Toffee-tastic</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>177</td>\n",
       "      <td>Thin Mints</td>\n",
       "      <td>117</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>177</td>\n",
       "      <td>Caramel deLites</td>\n",
       "      <td>84</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>177</td>\n",
       "      <td>Peanut Butter Patties</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>177</td>\n",
       "      <td>Girl Scout S'mores</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>177</td>\n",
       "      <td>Lemonades</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>177</td>\n",
       "      <td>Peanut Butter Sandwich</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>177</td>\n",
       "      <td>Shortbread</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>177</td>\n",
       "      <td>Thanks-A-Lot</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>177</td>\n",
       "      <td>Caramel Chocolate Chip</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>177</td>\n",
       "      <td>Toffee-tastic</td>\n",
       "      <td>77</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>177</td>\n",
       "      <td>Donations</td>\n",
       "      <td>20</td>\n",
       "      <td>*A kind soul just dropped off a $20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>143</td>\n",
       "      <td>Thin Mints</td>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>143</td>\n",
       "      <td>Caramel deLites</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>143</td>\n",
       "      <td>Peanut Butter Patties</td>\n",
       "      <td>No Sales</td>\n",
       "      <td>*Severe Peanut Allergy Impacted Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>143</td>\n",
       "      <td>Girl Scout S'mores</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>143</td>\n",
       "      <td>Lemonades</td>\n",
       "      <td>125</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>143</td>\n",
       "      <td>Peanut Butter Sandwich</td>\n",
       "      <td>No Sales</td>\n",
       "      <td>*Severe Peanut Allergy Impacted Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>143</td>\n",
       "      <td>Shortbread</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>143</td>\n",
       "      <td>Thanks-A-Lot</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>143</td>\n",
       "      <td>Caramel Chocolate Chip</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>143</td>\n",
       "      <td>Toffee-tastic</td>\n",
       "      <td>103</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>212</td>\n",
       "      <td>Thin Mints</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>212</td>\n",
       "      <td>Caramel deLites</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>212</td>\n",
       "      <td>Peanut Butter Patties</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>212</td>\n",
       "      <td>Girl Scout S'mores</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>212</td>\n",
       "      <td>Lemonades</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>212</td>\n",
       "      <td>Peanut Butter Sandwich</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>212</td>\n",
       "      <td>Shortbread</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>212</td>\n",
       "      <td>Thanks-A-Lot</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>212</td>\n",
       "      <td>Caramel Chocolate Chip</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>212</td>\n",
       "      <td>Toffee-tastic</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>179</td>\n",
       "      <td>Thin Mints</td>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>179</td>\n",
       "      <td>Caramel deLites</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>179</td>\n",
       "      <td>Peanut Butter Patties</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>179</td>\n",
       "      <td>Girl Scout S'mores</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>179</td>\n",
       "      <td>Lemonades</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>179</td>\n",
       "      <td>Peanut Butter Sandwich</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>179</td>\n",
       "      <td>Shortbread</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>179</td>\n",
       "      <td>Thanks-A-Lot</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>179</td>\n",
       "      <td>Caramel Chocolate Chip</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>179</td>\n",
       "      <td>Toffee-tastic</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Troop Number                     Box      Sold  \\\n",
       "0            179              Thin Mints        43   \n",
       "1            179         Caramel deLites        16   \n",
       "2            179   Peanut Butter Patties        22   \n",
       "3            179      Girl Scout S'mores        11   \n",
       "4            179               Lemonades        34   \n",
       "5            179  Peanut Butter Sandwich         8   \n",
       "6            179              Shortbread        38   \n",
       "7            179            Thanks-A-Lot        26   \n",
       "8            179  Caramel Chocolate Chip        17   \n",
       "9            179           Toffee-tastic        29   \n",
       "10           177              Thin Mints       117   \n",
       "11           177         Caramel deLites        84   \n",
       "12           177   Peanut Butter Patties        39   \n",
       "13           177      Girl Scout S'mores        33   \n",
       "14           177               Lemonades        56   \n",
       "15           177  Peanut Butter Sandwich        12   \n",
       "16           177              Shortbread        86   \n",
       "17           177            Thanks-A-Lot        24   \n",
       "18           177  Caramel Chocolate Chip        45   \n",
       "19           177           Toffee-tastic        77   \n",
       "20           177               Donations        20   \n",
       "21           143              Thin Mints        94   \n",
       "22           143         Caramel deLites        99   \n",
       "23           143   Peanut Butter Patties  No Sales   \n",
       "24           143      Girl Scout S'mores        76   \n",
       "25           143               Lemonades       125   \n",
       "26           143  Peanut Butter Sandwich  No Sales   \n",
       "27           143              Shortbread        36   \n",
       "28           143            Thanks-A-Lot        15   \n",
       "29           143  Caramel Chocolate Chip        69   \n",
       "30           143           Toffee-tastic       103   \n",
       "31           212              Thin Mints        21   \n",
       "32           212         Caramel deLites         7   \n",
       "33           212   Peanut Butter Patties         3   \n",
       "34           212      Girl Scout S'mores         5   \n",
       "35           212               Lemonades        15   \n",
       "36           212  Peanut Butter Sandwich         2   \n",
       "37           212              Shortbread        17   \n",
       "38           212            Thanks-A-Lot         9   \n",
       "39           212  Caramel Chocolate Chip        12   \n",
       "40           212           Toffee-tastic        13   \n",
       "41           179              Thin Mints        43   \n",
       "42           179         Caramel deLites        16   \n",
       "43           179   Peanut Butter Patties        22   \n",
       "44           179      Girl Scout S'mores        11   \n",
       "45           179               Lemonades        34   \n",
       "46           179  Peanut Butter Sandwich         8   \n",
       "47           179              Shortbread        38   \n",
       "48           179            Thanks-A-Lot        26   \n",
       "49           179  Caramel Chocolate Chip        17   \n",
       "50           179           Toffee-tastic        29   \n",
       "\n",
       "                                    Notes  \n",
       "0                                     NaN  \n",
       "1                                     NaN  \n",
       "2                                     NaN  \n",
       "3                                     NaN  \n",
       "4                                     NaN  \n",
       "5                                     NaN  \n",
       "6                                     NaN  \n",
       "7                                     NaN  \n",
       "8                                     NaN  \n",
       "9                                     NaN  \n",
       "10                                    NaN  \n",
       "11                                    NaN  \n",
       "12                                    NaN  \n",
       "13                                    NaN  \n",
       "14                                    NaN  \n",
       "15                                    NaN  \n",
       "16                                    NaN  \n",
       "17                                    NaN  \n",
       "18                                    NaN  \n",
       "19                                    NaN  \n",
       "20    *A kind soul just dropped off a $20  \n",
       "21                                    NaN  \n",
       "22                                    NaN  \n",
       "23  *Severe Peanut Allergy Impacted Sales  \n",
       "24                                    NaN  \n",
       "25                                    NaN  \n",
       "26  *Severe Peanut Allergy Impacted Sales  \n",
       "27                                    NaN  \n",
       "28                                    NaN  \n",
       "29                                    NaN  \n",
       "30                                    NaN  \n",
       "31                                    NaN  \n",
       "32                                    NaN  \n",
       "33                                    NaN  \n",
       "34                                    NaN  \n",
       "35                                    NaN  \n",
       "36                                    NaN  \n",
       "37                                    NaN  \n",
       "38                                    NaN  \n",
       "39                                    NaN  \n",
       "40                                    NaN  \n",
       "41                                    NaN  \n",
       "42                                    NaN  \n",
       "43                                    NaN  \n",
       "44                                    NaN  \n",
       "45                                    NaN  \n",
       "46                                    NaN  \n",
       "47                                    NaN  \n",
       "48                                    NaN  \n",
       "49                                    NaN  \n",
       "50                                    NaN  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, while \"Boxes Sold\" is almost always numeric, there are two instances where it takes the value \"No Sales\". (That seems to be because, based on the \"Notes\" column, a \"Severe Peanut Allergy Impacted Sales\".) Those two string values have caused the whole column to be cast as an `object` type. \n",
    "\n",
    "There also seem to be some other problems with the data. For example, it looks like there are some duplicates of Troop 179's data at the bottom of the dataset. We will address how to resolve this and the data type problem later on in the walkthrough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Print the data types of the `price_df` dataset below and the print the entire dataset to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price_df.dtypes)\n",
    "price_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Renaming Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sales_df` DataFrame has four columns - \"Troop Number\", \"Box\", \"Sold\", and \"Notes\". Some of these column names, however, are not that informative. For example, we may want to change \"Box\" to \"Cookie Type\" and \"Sold\" to \"Boxes Sold\". Let's do that by using the [`.rename()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) method, as below, and then use `.head()` to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.rename(columns={\"Box\" : \"Cookie Type\",\n",
    "                                    \"Sold\" : \"Boxes Sold\"})\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the column \"Box\" was changed to \"Cookie Type\" and \"Sold\" was changed to \"Boxes Sold\". \n",
    "\n",
    "The `.rename()` method can take multiple arguments, but the one most frequently used is the `columns` argument. Note that we haven't seen this type of argument notation before, but specific arguments can be specified using the following syntax `<argument name>=<value>`. Arguments specified in this way have a specific name, *keyword arguments*. All of Pandas' more complex methods rely on keyword arguments, so we will see this notation pop up more as we go along.\n",
    "\n",
    "In this case, we specify the keyword argument `columns` within the call to the `.rename()` method and pass a *dictionary* as the value of the argument. The dictionary is structured so that the *key* is the existing column name and the *value* is the new column name. This dictionary contains multiple key-value pairs corresponding to two column name changes we intend to make. The result of the `.rename()` method is a new DataFrame with the updated column names. In this case, we update the `sales_df` variable by assigning that new DataFrame to the existing variable `sales_df`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "The price variable in the `price_df` data is currently just called \"Dollars\". Rename that variable to \"Price (\\\\$)\" to make it more specific/informative. \n",
    "\n",
    "*Note: Pandas allows the use of special characters in column names!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = price_df.rename(columns={\"Dollars\" : \"Price ($)\"})\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Indexing Data\n",
    "\n",
    "How do you select just the data you want to work with? That can be done in Pandas using *indexing*, similar to how we used indexing to select specific values from lists in Intro to Python Part I. We will talk through all of the basics of indexing below as this is a crucial aspect of working with data in Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>The Index</ins>\n",
    "\n",
    "In each print out of the datasets above, there is an additional \"column\" on the left-hand side of the data that is yet to be explained. This is called the DataFrame *index* and is a unique ID associated with each observation in the dataset. The DataFrame is very similar to a list in the sense that its (default) index is zero-indexed and can be used to access specific data within the Python object. \n",
    "\n",
    "Unlike lists, however, observations are accessed with a specific DataFrame attribute called [`iloc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html), which stands for \"index locate\". Let's see how `.iloc` works below by printing `sales_df.iloc`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_df.iloc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see abvove that `sales_df.iloc` is an \"iLocIndexer object\", an object defined by the Pandas package. (That gobbledy-gook on the far right is the location of the object in memory in hexadecimal). You don't really need to worry much about this, but the most import part is that the \"iLocIndexer object\" can be indexed in the exact same way as a list. For example, we can access the first 5 observations of a DataFrame as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.iloc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! This gives us information identical to `sales_df.head()`. You can also get multiple specific non-sequential rows by passing a *list* of the desired rows within the square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.iloc[[1, 3, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you access just one row? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_df.iloc[0])\n",
    "print()\n",
    "print(type(sales_df.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that if we print just one observation, we get a print out of the individual values of the row as well as the \"Name\" or index of the row. If we check the type of the row using the `type()` function, we see that we no longer have a DataFrame but now have a Pandas [`Series`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html). \n",
    "\n",
    "Pandas stores individual rows or individual columns as Series, which you can think of as complex lists that have very helpful mathematical properties and defined methods. For example, the sum of two Series is not one longer Series, as it would be for lists, but rather the sum of two vectors as defined by vector algebra. \n",
    "\n",
    "*Note: You can print an empty line in Python by just calling* `print()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Combine what you just learned with your knowledge of list-based indexing to print the last 20 observations of the `sales_df` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Column and Combined Indexing</ins>\n",
    "\n",
    "It's important to note that in a DataFrame, not just observations have indices. Columns have indices as well, it just happens that when we read in our data, Pandas didn't use its default indices (0, 1, 2, 3, etc.), but gaved them *named indices*, in this case the column names stored in the first row of our csv data. Therefore, we can use the columns names to directly access specific columns by name using dictionary-like key indexing, as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales_df[\"Boxes Sold\"])\n",
    "print()\n",
    "print(type(sales_df[\"Boxes Sold\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with the individual row we accessed using the `iloc` object, here accessing just one column returns a Series object. Also, like with `iloc` indexing, we can access multiple columns by passing a list of column names as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df[[\"Cookie Type\", \"Boxes Sold\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to index a dataset based of of both columns and observations? Say we want to see only the first 10 observations of the \"Cookie Type\" and \"Boxes Sold\" columns? In that case, we'd use the [`.loc`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html) indexer, which indexes a DataFrame based on the name of its indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.loc[:10, [\"Cookie Type\", \"Boxes Sold\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, we take advantage of the fact that the *name* of our row/observation index is the same as its position, which is the case by default. Therefore, we specify our doubly-indexed DataFrame by specifying the observations we want first (in this case by specifying a *slice* or range of observations), followed by a comma, followed by a *list* of our desired columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Use the `.loc` indexer to access the 1st, and 21st, and 27th observations and just the \"Troop Number\" and \"Notes\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.loc[[0, 20, 26], [\"Troop Number\", \"Notes\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Boolean Indexing</ins>\n",
    "\n",
    "While good to know, it's sometimes not useful/pragmatic to access indiviual rows of a DataFrame using positional indexing. More often than not, it's more useful to be able to access rows based on a characteristic they share, such a specific value or condition. Indexing a DataFrame in that way is possible using boolean indexing.\n",
    "\n",
    "For example, you might want to look at all data for Troop 177 Only. Rather identify the position of each observation assocaite with Troop 177 and then selecting observations using positional indexing, we can directly index by doing the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df[sales_df[\"Troop Number\"] == 177]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, rather than specify specific indices associated with the observations we want to isolate, we instead pass a comparison, in this case a comparison of a specific column against a specific value. In Python Part I, we saw that the result of a comparison like `5 < 6` is a boolean value. What does this comparison of a Pandas Series against a value return? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df[\"Troop Number\"] == 177"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparison involving an individual column returns another Series, in this case a Series of type `bool`. Specifically, this series takes the value `True` whenever the compairson is True and `False` otherwise. In Pandas, this kind of Series that references specific observations using boolean values is called a *boolean mask*. A boolean mask, then, can be used to *boolean index* the dataset as a boolean value of `True` indiciates that we'd like to select the observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Use boolean indexing to select just the observations from `sales_df` associated with \"Toffee-tastic\" cookies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.loc[sales_df[\"Cookie Type\"] == \"Toffee-tastic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cleaning Data\n",
    "\n",
    "Up to this point, we have read in data, explored it, renamed columns (or \"edited metadata\" if you want to be fancy), and learned how to select just the data we want to work with. But now we will cover how you clean data so that we can finally analyze it.\n",
    "\n",
    "Cleaning data is a broad umbrella, but can include changing variable types, dropping superfluous columns and unneeded observations, removing duplicate data, and much more. We will look into how to do many of these tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Replace Observations</ins>\n",
    "\n",
    "When we looked at the sales data earlier, we noticed that the \"Boxes Sold\" column was an `object` type because it had a couple of instances where it took the value \"No Sales\", therby preventing it from being read in as a numeric variable. We'd like to calculate some statistics based off the number of \"Boxes Sold\", so we'll need to convert \"Boxes Sold\" to a numeric variable type, we'll need to either replace these observations with a numeric value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's used what we learned about indexing to change the value of those \"No Sales\" observations to 0. Just as positional indexing can be used to change the value of specific objects within a list in Python, boolean indexing can be used to change tha value of specific observations in Pandas. For example, we can update the value of \"Boxes Sold\" by doing the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.loc[sales_df[\"Boxes Sold\"] == \"No Sales\", \"Boxes Sold\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we use boolean indexing to select just the rows we want to modify and which column we want to make changes to. We then set those observations to 0 by simply writing `= 0`. \n",
    "\n",
    "We can then print the dataset to see the updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "A similar problem  as encountered above occured for the price data, `price_df`, that you created earlier. In this instance, one observation of the \"Price (\\\\$)\" variable was listed as \"\\\\$5.00\" rather than just 5, causing the column to be read-in as a string. So, in order to be able to covert this column to a numeric variable type, use boolean indexing to set the price of \"caramel chocolate chip\" cookies to the numeric value `5`. Then print `price_df` to make sure your changes were implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.loc[price_df[\"Cookie\"] == \"caramel chocolate chip\", \"Price ($)\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Change Column Data Types</ins>\n",
    "\n",
    "Now that the \"Boxes Sold\" column in `sales_df` has only numeric values, we can change the column to a numeric data type, such as `int` or `float`. Since the data are all integers, we'll pick `int`.\n",
    "\n",
    "In order to change the column type, we can use the [`.astype()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html) method. The `.astype()` method is a method associated with the Series object that converts a column of one type to another type based on a basic Python variable type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df[\"Boxes Sold\"] = sales_df[\"Boxes Sold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we use the `.astype()` method in combination with the `int` basic Python variable type to convert the \"Boxes Sold\" column to an integer.\n",
    "\n",
    "We can also convert numeric columns into the `object` type. For example, we may want the \"Troop Number\" column to be a string since we don't plan on doing any arithmetic with the troop number. We can do that as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df[\"Troop Number\"] = sales_df[\"Troop Number\"].astype(str)\n",
    "sales_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Convert the \"Price (\\\\$)\" column to an integer using the `.astype()` method. Then print the `.dtypes` of the `price_df` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df[\"Price ($)\"] = price_df[\"Price ($)\"].astype(int)\n",
    "price_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Drop Duplicates</ins>\n",
    "\n",
    "As we also saw above, `sales_df` has some duplicate observations. Luckily, working with duplicate observations is a simple task in Pandas. For example, we can quickly identify duplicate observations using the [`.duplicated()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html) method, as below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df[sales_df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a couple things in the code above. First, the `duplicated()` method returns a boolean mask that identifies which observations are duplicates. In this case, `duplicated()` only identifes the second (or later) instance of an observation; it does not return all observations for which there are duplicates. Another way to think about this is that `duplicated()` only returns the excess or superfluous observations, i.e. the ones that would be dropped if you wished to drop duplicate observations. \n",
    "\n",
    "Also note that we introduced a new syntax here, in this case a shorthand for row indexing. In the above code, we supplied a boolean mask inside of square brackets adjoining our DataFrame and this returned just the rows associated we wanted to view. While this allows you to quickly view select rows, make sure to always use the `.loc` method when selecting specific rows *and* specific columns.\n",
    "\n",
    "How do we drop these duplicates? We can easily do that with the [`drop_duplicates()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the `.drop_duplicates()` method dropped the observations indicated by the `.duplicated()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Use the `.duplicated()` and `.drop_duplicates()` method to first identify and then drop duplicates in `price_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df[price_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = price_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Dropping Observations and Columns</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll look at how to drop DataFrame observations and columns.\n",
    "\n",
    "In `sales_df`, we can see that the Cookie Type of observation 20 is \"Donations\" and has an accompanying note that \"A kind soul just dropped off a \\\\$20\". Let's drop that observation so that the dataset is limited to only cookie sales. \n",
    "\n",
    "We can easily drop observations using boolean indexing as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df[sales_df[\"Cookie Type\"] != \"Donations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we limit the data to just observations where \"Cookie Type\" doesn't equal \"Donations\". Then, we replace the existing `sales_df` DatFrame with that limited DataFrame to solidify the changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns, we can either specify just the columns we intend to keep or the ones we intend to drop. To limit to just the columns we intend to keep, we can use the syntax we encountered earlier and pass a *list* of the columns we want to keep within the DataFrame indexer. For example, if we wanted to keep just \"Cookie Type\" and \"Boxes Sold\", we could say\n",
    "\n",
    "```python\n",
    "sales_df = sales_df[[\"Cookie Type\", \"Boxes Sold\"]]\n",
    "```\n",
    "\n",
    "If we have many columns and only want to drop one or two of them, however, this method can be a bit arduous. In that instance, rather, we should use the [`.drop()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method to specify just the columns we'd want to drop. For example, in `sales_df`, we can go ahead and drop the \"Notes\" column by doing the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.drop(columns=[\"Notes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we call the .`drop()` method and then pass a list of column names we want to drop to the named keyword argument `columns`. This drops the specified columns and the we assign the output to the variable `sales_df` to overwrite the DataFrame.\n",
    "\n",
    "We can check that this worked by printing the list of the existing columns. We can do this in the Pandas using a new attribute we haven't encountered yet called the `.columns` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we seem the \"Notes\" columns has been dropped from the DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Use boolean indexing to limit `sales_df` to observations with positive non-zero values of \"Boxes Sold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df[sales_df[\"Boxes Sold\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Using Built-In Methods</ins>\n",
    "\n",
    "There is one dataset we haven't read in yet called \"scout_counts.csv\" that gives us the number of scouts associated with each Troop. Let's read in and view that data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scout_url = \"https://raw.githubusercontent.com/cra-international/Intro-to-Python/master/Intro%20to%20Python%20-%20II/scout_counts.csv\"\n",
    "scout_df = pd.read_csv(scout_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scout_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we have the number of scouts in a column called \"Scouts\" and the troop in a column named \"Troop Name\". Ultimately, we'd like to merge this data with the sales data, but in order to do so we need to remove the `\"Troop \"` substring from the values in the \"Troop Name\" column. If we were trying to do this on one string, we might use string indexing or the string's built-in `.replace()` method to do this. Luckily, we are able to access vectorized versions of these operations using the [`.str`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.html?highlight=series%20str#pandas.Series.str) attribute as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scout_df[\"Indexed Col\"] = scout_df[\"Troop Name\"].str[-3:]\n",
    "scout_df[\"Replaced Col\"] = scout_df[\"Troop Name\"].str.replace(\"Troop \", \"\")\n",
    "scout_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that the `.str` attribute we are able to apply [string indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#indexing-with-str) and the string [`.replace()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.replace.html) method to every value in the \"Troop Name\" column.\n",
    "\n",
    "Nearly every string method is accessible using the `.str` attribute. Although we won't touch on Datetime types, Datetime methods are accessible using the `.dt` attribute in Pandas.\n",
    "\n",
    "In the code below, we'll drop one of the columns we just created and the other so we can use it for to merge the scout data with the sales data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scout_df = scout_df.rename(columns={\"Indexed Col\" : \"Troop Number\"})\n",
    "scout_df = scout_df.drop(columns=[\"Replaced Col\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scout_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "The \"Cookie\" column in `price_df` is all lower case. We'll also want to merge this data with `sales_df` so use the `.str` attribute to lowercase the values in the \"Cookie Type\" column in `sales_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df[\"Cookie Type\"] = sales_df[\"Cookie Type\"].str.lower()\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Merging Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have clean data, we are able to create more comprehensive datasets by merging datasets together. For example, we would like to merge the sales and price data together so that we can calculate the revenue associated with each product for each Troop. We can do that by using the [`.merge()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) method. The `.merge()` method has a number of keyword arguments that we will explore that allow you to determine exactly how the datasets are merged. We will show how to merge `sales_df` and `price_df`and then explain below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = sales_df.merge(price_df, how=\"outer\", left_on=\"Cookie Type\", right_on=\"Cookie\", indicator=True, validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, note that `.merge()` is called from `sales_df` and the first argument is the DataFrame to merge with, in this case `price_df`. We then specify how we'd like to merge the datasets using the `how` statement. There are four options: \"left\", \"right\", \"outer\", and \"inner\". Those familiar with SQL should be familiar with these. In this instance, the DataFrame from which the `merge()` method was called is the \"left\" dataset and the DataFrame specified inside the `merge()` statement is the \"right\" DataFrame. If you do a \"left\" merge by specifying `how=\"left\"`, then all observations from the \"left\" DataFrame are kept and observations that don't merge from the \"right\" DataFrame are dropped. A \"right\" merge is the flipside of that. An \"inner\" merge, then, only keeps merged observations while an \"outer\" merge keeps all observations, regardless of where they are from or if they merged. \n",
    "\n",
    "Next, we specify the merge keys. Those familiar with Stata are used to the variables needing to be the same name prior to the merge. In Pandas, however, we can simply specify which column in the \"left\" DataFrame to use as the merge key and which column in the \"right\" DataFrame to use as its merge key. We can do that by specifying `left_on` and `right_on`, respectively. If the merging column is the same name in both DataFrames, then that column can be specified with the `on` argument and `left_on` and `right_on` no longer need to be specified.\n",
    "\n",
    "Then, we specify `indicator=True` which creates a column called \"\\_merge\" by default and *indicates* which dataset the observation originated in. If the observation is the result of a successful merge, it came from \"both\" datasets, otherwise it could come from the \"left\" DataFrame only or the \"right\" DataFrame. Stata users should be familiar with this syntax.\n",
    "\n",
    "Finally, we specify validate=\"m:1\" which checks that the merge key is \"many\" or non-unique in the \"left\" DataFrame and unique in the \"right\" DataFrame. If this is not the case, the function with throw an error. It is good practice to specify the `validate` argument as many-to-many merges are defined in Pandas which may lead to unexpected results. Again, users with some Stata experience should be familiar with this syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some of the data from `price_df` didn't merge with `sales_df` because \"Peanut Butter Sandwich\" was mispelled in `price_df`. So, we can fix that and then rerun the merge and make sure all values merge as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df.loc[price_df[\"Cookie\"] == \"peanut butter sanwich\", \"Cookie\"] = \"peanut butter sandwich\"\n",
    "price_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = sales_df.merge(price_df, how=\"outer\", left_on=\"Cookie Type\", right_on=\"Cookie\", indicator=True, validate=\"m:1\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, it looks like everything merged successfully. Now, we can drop the extra key variable \"Cookie\" and the indicator \"\\_merge\" so that we are left with a clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=[\"_merge\", \"Cookie\"])\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Use \"Troop Number\" to merge `scout_df` with `merged_df` and create a new DataFrame, `analysis_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = merged_df.merge(scout_df, how=\"outer\", on=\"Troop Number\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Analyzing Data\n",
    "\n",
    "### <ins>Arithmetic Operators</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a clean dataset with that we can now do some analysis on. For example we can calculate revenues for each cookie type by multiplying price by quantity sold, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[\"Cookie Revenues\"] = analysis_df[\"Boxes Sold\"] * analysis_df[\"Price ($)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, Python arithmetic operators are defined for Pandas columns/Series allowing us to easily work with numeric Pandas columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Using `analysis_df`, divide \"Cookie Revenues\" by \"Scouts\" to create \"Revenue per Scout\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[\"Revenue per Scout\"] = analysis_df[\"Cookie Revenues\"]/analysis_df[\"Scouts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Sorting Data</ins>\n",
    "\n",
    "We can also sort data to quickly get a sense of rankings. For example, we can use the [`.sort_values()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) method to sort by \"Troop Number\" and (descending) by \"Boxes Sold\" to see which type of cookie each troop sold the most of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.sort_values(by=['Troop Number', \"Boxes Sold\"], ascending=(True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we specify each of our sort columns in a list in the order we want to sort using the `by` argument. We then specify a tuple of the same length as the list of columns to determined whether each column is sorted in ascending (`True`) or descending (`False`) order using the `ascending` argument. By default all values are sorted in ascending order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Use `.sort_values()` to sort by \"Cookie Type\" (ascending) and \"Revenue per Scout\" (descending). Then, look at the data to identify which Troop sold the most \"Thin Mints\" in terms of \"Revenue per Scout\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.sort_values(by=['Cookie Type', \"Revenue per Scout\"], ascending=(True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Summary Stats</ins>\n",
    "\n",
    "It's also pretty easy to [calculate basic summary stats](https://pandas.pydata.org/docs/getting_started/intro_tutorials/06_calculate_statistics.html) in Pandas. For example, we can use the `.describe()` method to get pre-defined summary statistics for each numeric variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These summary statistics can also be computed directly using built-in functions, such as `.mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[[\"Boxes Sold\", \"Cookie Revenues\", \"Revenue per Scout\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Use the [`.sum()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html) function to calculate the total number of \"Boxes Sold\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df[\"Boxes Sold\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Aggregating/Grouping Data</ins>\n",
    "\n",
    "Often it is useful to calculate summary statistics within groups or collapse/aggregate data based on a category. That can be done using Pandas' `groupby()` command. For example, we can calculate mean \"Revenue per Scout\" by troop by doing the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.groupby(\"Troop Number\")[\"Revenue per Scout\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even calculate all of the statistics associated with the `.describe()` method for \"Revenue per Scout\" by changing `.mean()` to `.describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.groupby(\"Troop Number\")[\"Revenue per Scout\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate sums within each group using the `.sum()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.groupby(\"Troop Number\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to calculate different statistics for different variables? For example, we might want to take the mean of \"Scouts\" since that would accurately reflect the number of scouts within each troop as opposed to the sum. We do that using the [`.agg()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html) method as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df.groupby(\"Troop Number\").agg({'Scouts': ['mean'],\n",
    "                                         'Cookie Revenues': ['sum'],\n",
    "                                         'Revenue per Scout': ['sum']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.agg()` method takes a dictionary as an input where each key of the dictionary is a column and the value is a list of statistics to calculate for that column. \n",
    "\n",
    "*Note: What is returned is a multi-indexed DataFrame. We have not encountered a multi-indexed DataFrame yet and a full treatment of multi-indexed DataFrames is outside the scope of this Tech Lab. However, know that in the above example, the sum of \"Revenue per Scout\" would be accessed as* `analysis_df[\"Revenue per Scout\"][\"sum\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now You Try!\n",
    "\n",
    "Use `analysis_df` to calculate total \"Cookie Revenues\" by \"Cookie Type\". Which cookie was the highest grossing cookie? Then, calculate a cookie-level dataset with total \"Boxes Sold\", the price of the cookie, and the total \"Cookie Revenues\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cookie Type\n",
       "caramel chocolate chip     715\n",
       "caramel delites            824\n",
       "girl scout s'mores         500\n",
       "lemonades                  920\n",
       "peanut butter patties      256\n",
       "peanut butter sandwich      88\n",
       "shortbread                 708\n",
       "thanks-a-lot               296\n",
       "thin mints                1100\n",
       "toffee-tastic              888\n",
       "Name: Cookie Revenues, dtype: int32"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df.groupby(\"Cookie Type\")['Cookie Revenues'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Boxes Sold</th>\n",
       "      <th>Price ($)</th>\n",
       "      <th>Cookie Revenues</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cookie Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>caramel chocolate chip</th>\n",
       "      <td>143</td>\n",
       "      <td>5</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caramel delites</th>\n",
       "      <td>206</td>\n",
       "      <td>4</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girl scout s'mores</th>\n",
       "      <td>125</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemonades</th>\n",
       "      <td>230</td>\n",
       "      <td>4</td>\n",
       "      <td>920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peanut butter patties</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peanut butter sandwich</th>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shortbread</th>\n",
       "      <td>177</td>\n",
       "      <td>4</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thanks-a-lot</th>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thin mints</th>\n",
       "      <td>275</td>\n",
       "      <td>4</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toffee-tastic</th>\n",
       "      <td>222</td>\n",
       "      <td>4</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Boxes Sold Price ($) Cookie Revenues\n",
       "                              sum      mean             sum\n",
       "Cookie Type                                                \n",
       "caramel chocolate chip        143         5             715\n",
       "caramel delites               206         4             824\n",
       "girl scout s'mores            125         4             500\n",
       "lemonades                     230         4             920\n",
       "peanut butter patties          64         4             256\n",
       "peanut butter sandwich         22         4              88\n",
       "shortbread                    177         4             708\n",
       "thanks-a-lot                   74         4             296\n",
       "thin mints                    275         4            1100\n",
       "toffee-tastic                 222         4             888"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df.groupby(\"Cookie Type\").agg({\"Boxes Sold\" : [\"sum\"],\n",
    "                                        \"Price ($)\" : [\"mean\"],\n",
    "                                        \"Cookie Revenues\" : [\"sum\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Exporting Data\n",
    "\n",
    "Awesome, you've read in, cleaned, merged, and analyzed data all with Pandas! Now that you have some cool results, how do you export them? Pandas can write to almost any file type (except SAS, since its proprietary). For example, you can write the analysis DataFrame to an Excel file by using [`to_excel()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html)\n",
    "\n",
    "```python\n",
    "analysis_df.to_excel(\"Path to File\\analysis_df.xlsx\", index=False)\n",
    "```\n",
    "\n",
    "or you could write to a Stata file by using [`to_stata()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_stata.html)\n",
    "\n",
    "```python\n",
    "analysis_df.to_stata(\"Path to File\\analysis_df.dta\", write_index=False)\n",
    "```\n",
    "\n",
    "*Note: In the above code, the* `index=False` *and* `write_index=False` *specifications make sure Pandas doesn't add the row index as an extra column to your final dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to save you DataFrame as a Python object without converting it to another file type? You can save you DataFrame as a serialized Python object called a *pickle* using the [`to_pickle`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html) command, like so\n",
    "\n",
    "```python\n",
    "analysis_df.to_pickle(\"Path to File\\analysis_df.pkl\")\n",
    "```\n",
    "\n",
    "You can then read in the data later on using the [`pd.read_pickle()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_pickle.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
